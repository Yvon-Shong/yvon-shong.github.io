---
title: Recurrent/Recursive Neural Network
date: 2017-09-10 19:05:39
categories: code
tags: 
- ml
toc: true
---
循环/递归神经网络
<!-- more -->
# RNN

递归神经网络（RNN）是两种人工神经网络的总称：时间递归神经网络（recurrent neural network）和结构递归神经网络（recursive neural network）。

- 时间递归神经网络的神经元间连接构成有向图，
- 而结构递归神经网络利用相似的神经网络结构递归构造更为复杂的深度网络。

RNN一般指代时间递归神经网络。单纯递归神经网络因为无法处理随着递归，权重指数级爆炸或消失的问题（Vanishing gradient problem），难以捕捉长期时间关联；而结合不同的LSTM可以很好解决这个问题。时间递归神经网络可以描述动态时间行为，因为和前馈神经网络（feedforward neural network）接受较特定结构的输入不同，RNN将状态在自身网络中循环传递，因此可以接受更广泛的时间序列结构输入。



CNN是空间上做卷积，RNN是在做卷积的同时在同层自循环。本质上是在模拟人类更复杂的神经网络。

![](http://upload-images.jianshu.io/upload_images/1667471-4eba217f653527d5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

输出层是一个全连接层，它的每个节点都和隐藏层的每个节点相连，
隐藏层是循环层。

由于循环，可以看作任意多个输入值。



# RNN 主要处理序列数据

RNN主要解决序列数据的处理，比如文本、语音、视频等等。这类数据的样本间存在顺序关系，每个样本和它之前的样本存在关联。比如说，在文本中，一个词和它前面的词是有关联的；在气象数据中，一天的气温和前几天的气温是有关联的。一组观察数据定义为一个序列，从分布中可以观察出多个序列。

一个序列 $X=\{  x_1,x_2,...,x_N \}$  的最简单模型为 $P(X)=\Pi _{i=1}^N P(x_i|x_1,...,x_{i-1})$

也就是说，序列里的每一个元素都和排在它前面的所有元素直接相关。当然，这个模型存在致命的问题：它的复杂度$O(N!)$会爆炸性增长

RNNs已经被在实践中证明对NLP是非常成功的。如词向量表达、语句合法性检查、词性标注等。在RNNs中，目前使用最广泛最成功的模型便是[LSTMs(Long Short-Term Memory，长短时记忆模型)模型](https://en.wikipedia.org/wiki/Long_short_term_memory)，该模型通常比vanilla RNNs能够更好地对长短时依赖进行表达，该模型相对于一般的RNNs，只是在隐藏层做了手脚。









# 训练

对于RNN是的训练和对传统的ANN训练一样。同样使用BP误差反向传播算法，不过有一点区别。如果将RNNs进行网络展开，那么参数W,U,V是共享的，而传统神经网络却不是的。并且在使用梯度下降算法中，每一步的输出不仅依赖当前步的网络，并且还以来前面若干步网络的状态。比如，在t=4时，我们还需要向后传递三步，已经后面的三步都需要加上各种的梯度。该学习算法称为Backpropagation Through Time (BPTT)。后面会对BPTT进行详细的介绍。需要意识到的是，在vanilla RNNs训练中，[BPTT无法解决长时依赖问题](http://www.jmlr.org/proceedings/papers/v28/pascanu13.pdf)(即当前的输出与前面很长的一段序列有关，一般超过十步就无能为力了)，因为BPTT会带来所谓的梯度消失或梯度爆炸问题(the vanishing/exploding gradient problem)。当然，有很多方法去解决这个问题，如LSTMs便是专门应对这种问题的。



# RNN变种

## [Simple RNNs(SRNs)

SRNs是RNNs的一种特例，它是一个三层网络，并且在隐藏层增加了上下文单元，下图中的y便是隐藏层，u便是上下文单元。上下文单元节点与隐藏层中的节点的连接是固定(谁与谁连接)的，并且权值也是固定的(值是多少)，其实是一个上下文节点与隐藏层节点一一对应，并且值是确定的。在每一步中，使用标准的前向反馈进行传播，然后使用学习算法进行学习。上下文每一个节点保存其连接的隐藏层节点的上一步的输出，即保存上文，并作用于当前步对应的隐藏层节点的状态，即隐藏层的输入由输入层的输出与上一步的自己的状态所决定的。因此SRNs能够解决标准的多层感知机(MLP)无法解决的对序列数据进行预测的任务。 

![](http://img.blog.csdn.net/20150921230335967)



##Bidirectional RNNs

  Bidirectional RNNs(双向网络)的改进之处便是，假设当前的输出(第t步的输出)不仅仅与前面的序列有关，并且还与后面的序列有关。例如：预测一个语句中缺失的词语那么就需要根据上下文来进行预测。Bidirectional RNNs是一个相对较简单的RNNs，是由两个RNNs上下叠加在一起组成的。输出由这两个RNNs的隐藏层的状态决定的。如下图所示： 

![](http://img.blog.csdn.net/20150921230410290)

## Deep(Bidirectional)RNNs

Deep(Bidirectional)RNNs与Bidirectional RNNs相似，只是对于每一步的输入有多层网络。这样，该网络便有更强大的表达与学习能力，但是复杂性也提高了，同时需要更多的训练数据。Deep(Bidirectional)RNNs的结构如下图所示： 

![](http://img.blog.csdn.net/20150921230507091)

## Echo State Networks

  ESNs(回声状态网络)虽然也是一种RNNs，但是它与传统的RNNs相差很大。ESNs具有三个特点：

- 它的核心结构时一个随机生成、且保持不变的储备池(Reservoir)，储备池是大规模的、随机生成的、稀疏连接(SD通常保持1%～5%，SD表示储备池中互相连接的神经元占总的神经元个数N的比例)的循环结构；
- 其储备池到输出层的权值矩阵是唯一需要调整的部分；
- 简单的线性回归就可完成网络的训练。

  从结构上讲，ESNs是一种特殊类型的循环神经网络，其基本思想是：使用大规模随机连接的循环网络取代经典神经网络中的中间层，从而简化网络的训练过程。因此ESNs的关键是中间的储备池。网络中的参数包括：W为储备池中节点的连接权值矩阵，$W_{in}$为输入层到储备池之间的连接权值矩阵，表明储备池中的神经元之间是连接的，$W_{back}$为输出层到储备池之间的反馈连接权值矩阵，表明储备池会有输出层来的反馈，$W_{out}$输入层、储备池、输出层到输出层的连接权值矩阵，表明输出层不仅与储备池连接，还与输入层和自己连接。$W^{out}_{bias}$表示输出层的偏置项。 
  对于ESNs，关键是储备池的四个参数，如储备池内部连接权谱半径SR(SR=λmax=max{|W的特征指|}，只有SR <1时，ESNs才能具有回声状态属性)、储备池规模N(即储备池中神经元的个数)、储备池输入单元尺度IS(IS为储备池的输入信号连接到储备池内部神经元之前需要相乘的一个尺度因子)、储备池稀疏程度SD(即为储备池中互相连接的神经元个数占储备池神经元总个数的比例)。对于IS，如果需要处理的任务的非线性越强，那么输入单元尺度越大。该原则的本质就是通过输入单元尺度IS，将输入变换到神经元激活函数相应的范围(神经元激活函数的不同输入范围，其非线性程度不同)。 
  ESNs的结构如下图所示： 

![](http://img.blog.csdn.net/20150921230739897)

## Gated Recurrent Unit Recurrent Neural Networks

  GRUs也是一般的RNNs的改良版本，主要是从以下两个方面进行改进。一是，序列中不同的位置处的单词(已单词举例)对当前的隐藏层的状态的影响不同，越前面的影响越小，即每个前面状态对当前的影响进行了距离加权，距离越远，权值越小。二是，在产生误差error时，误差可能是由某一个或者几个单词而引发的，所以应当仅仅对对应的单词weight进行更新。GRUs的结构如下图所示。GRUs首先根据当前输入单词向量word vector已经前一个隐藏层的状态hidden state计算出update gate和reset gate。再根据reset gate、当前word vector以及前一个hidden state计算新的记忆单元内容(new memory content)。当reset gate为1的时候，new memory content忽略之前的所有memory content，最终的memory是之前的hidden state与new memory content的结合。 

![](http://img.blog.csdn.net/20150921230927637)



## LSTM Netwoorks

 LSTMs与GRUs类似，目前非常流行。它与一般的RNNs结构本质上并没有什么不同，只是使用了不同的函数去去计算隐藏层的状态。在LSTMs中，i结构被称为cells，可以把cells看作是黑盒用以保存当前输入xt之前的保存的状态ht−1，这些cells更加一定的条件决定哪些cell抑制哪些cell兴奋。它们结合前面的状态、当前的记忆与当前的输入。已经证明，该网络结构在对长序列依赖问题中非常有效。LSTMs的网络结构如下图所示。

![](http://img.blog.csdn.net/20150921230954367)

### LSTMs解决的问题也是GRU中所提到的问题，如下图所示： 

![](http://img.blog.csdn.net/20150921231113097)



### LSTMs与GRUs的区别如图所示

![](http://img.blog.csdn.net/20150921231146716)

从上图可以看出，它们之间非常相像，不同在于：

- new memory的计算方法都是根据之前的state及input进行计算，但是GRUs中有一个reset gate控制之前state的进入量，而在LSTMs里没有这个gate；
- 产生新的state的方式不同，LSTMs有两个不同的gate，分别是forget gate (f gate)和input gate(i gate)，而GRUs只有一个update gate(z gate)；
- LSTMs对新产生的state又一个output gate(o gate)可以调节大小，而GRUs直接输出无任何调节。



## Clockwork RNNs(CW-RNNs)

CW-RNNs也是一个RNNs的改良版本，是一种使用时钟频率来驱动的RNNs。它将隐藏层分为几个块(组，Group/Module)，每一组按照自己规定的时钟频率对输入进行处理。并且为了降低标准的RNNs的复杂性，CW-RNNs减少了参数的数目，提高了网络性能，加速了网络的训练。CW-RNNs通过不同的隐藏层模块工作在不同的时钟频率下来解决长时间依赖问题。将时钟时间进行离散化，然后在不同的时间点，不同的隐藏层组在工作。因此，所有的隐藏层组在每一步不会都同时工作，这样便会加快网络的训练。并且，时钟周期小的组的神经元的不会连接到时钟周期大的组的神经元，只会周期大的连接到周期小的(认为组与组之间的连接是有向的就好了，代表信息的传递是有向的)，周期大的速度慢，周期小的速度快，那么便是速度慢的连速度快的，反之则不成立。现在还不明白不要紧，下面会进行讲解。 
   CW-RNNs与SRNs网络结构类似，也包括输入层(Input)、隐藏层(Hidden)、输出层(Output)，它们之间也有向前连接，输入层到隐藏层的连接，隐藏层到输出层的连接。但是与SRN不同的是，隐藏层中的神经元会被划分为若干个组，设为g，每一组中的神经元个数相同，设为k，并为每一个组分配一个时钟周期Ti∈{T1,T2,...,Tg}，每一个组中的所有神经元都是全连接，但是组j到组i的循环连接则需要满足Tj大于Ti。如下图所示，将这些组按照时钟周期递增从左到右进行排序，即T1<T2<...<Tg，那么连接便是从右到左。例如：隐藏层共有256个节点，分为四组，周期分别是[1,2,4,8]，那么每个隐藏层组256/4=64个节点，第一组隐藏层与隐藏层的连接矩阵为64*64的矩阵，第二层的矩阵则为64*128矩阵，第三组为64*(3*64)=64*192矩阵，第四组为64*(4*64)=64*256矩阵。这就解释了上一段的后面部分，速度慢的组连到速度快的组，反之则不成立。 
  CW-RNNs的网络结构如下图所示：

![](http://img.blog.csdn.net/20150921231441483)





## Gradient Vanishing/Exploding (梯度消失和梯度爆炸)

RNN训练困难的主要原因在于隐藏层参数 w 的传播：由于误差传播在展开后的RNN上，无论在前向传播过程还是在反向传播过程中 w 都会乘上多次，这就导致：

- 梯度消失：如果梯度很小的话（<1），乘上多次指数级下降，对输出几乎就没有影响了
- 梯度爆炸：反过来，如果梯度很大的话，乘上多次指数级增加，又导致了梯度爆炸

当然了，这个问题其实存在于任何深度神经网络中，只是由于RNN的递归结构导致其尤其明显。

对于**梯度爆炸**问题，可以通过截断的方式来有效避免：

![](https://cloud.githubusercontent.com/assets/676637/21475489/245c4948-cb67-11e6-8ebb-c755e4a34e94.png)



而对**梯度消失**问题，则有很多不同的方案：

1. 有效初始化+ReLU激活函数能够得到较好效果
2. 算法上的优化，例如截断的BPTT算法。
3. 模型上的改进，例如LSTM、GRU单元都可以有效解决长期依赖问题。
4. 在BPTT算法中加入skip connection，此时误差可以间歇的向前传播。
5. 加入一些Leaky Units，思路类似于skip connection





# LSTM

LSTM 全称叫 Long Short-Term Memory networks，它和传统 RNN 唯一的不同就在与其中的神经元（感知机）的构造不同。传统的 RNN 每个神经元和一般神经网络的感知机没啥区别，但在 LSTM 中，每个神经元是一个“记忆细胞”（元胞状态，Cell State），将以前的信息连接到当前的任务中来。每个LSTM细胞里面都包含

- 输入门（input gate）: 一个Sigmoid层，观察 $h_{t-1}$ 和 $x_t$ ，对于元胞状态 $c_{t-1}$ 中的每一个元素，输出一个0~1之间的数。1表示“完全保留该信息”，0表示“完全丢弃该信息”：$  f_t=σ(W^f x_t+U^f h_{t−1})$ 
- 遗忘门（forget gate): 一个Sigmoid层决定我们要更新哪些信息，并由一个tanh层创造了一个新的候选值（结果在(−1,1)范围） $i_t=σ(W^i x_t+U^i h_{t−1})$  ; $ \tilde{c}_t=\tanh(W^cx_t+U^ch_{t-1})$ ;  $c_t=f_t∘c_t−1+i_t∘\tilde{c}_t$ 


- 输出门（output gate）：控制哪些信息需要输出 $o_t=σ(W^o x_t+U^o h_{t−1})$ ; $h_t =o_t∘tanh(c_t) $ 

![](http://feisky.xyz/machine-learning/_images/14800648468281.jpg)



典型的工作流为：在“输入门”中，根据当前的数据流来控制接受细胞记忆的影响；接着，在 “遗忘门”里，更新这个细胞的记忆和数据流；然后在“输出门”里产生输出更新后的记忆和数据流。LSTM 模型的关键之一就在于这个“遗忘门”， 它能够控制训练时候梯度在这里的收敛性（从而避免了 RNN 中的梯度 vanishing/exploding 问题），同时也能够保持长期的记忆性。

如果我们把LSTM的forget gate全部置0（总是忘记之前的信息），input gate全部 置1，output gate全部置1（把cell state中的信息全部输出），这样LSTM就变成一个标准的RNN。